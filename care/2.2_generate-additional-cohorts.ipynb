{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to the pan-disease and first-degree-MSS cohort, generate two personalized cohorts to determine the N-of-1 sample's outliers.\n",
    "\n",
    "Cohorts are :\n",
    "- N-of-1 diagnosed disease (when available)\n",
    "- First and second degree most similar samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import errno\n",
    "import logging\n",
    "\n",
    "# Setup: load conf, retrieve sample ID, logging\n",
    "with open(\"conf.json\",\"r\") as conf:\n",
    "    c=json.load(conf)\n",
    "sample_id = c[\"sample_id\"]    \n",
    "print(\"Running on sample: {}\".format(sample_id))\n",
    "\n",
    "logging.basicConfig(**c[\"info\"][\"logging_config\"])\n",
    "logging.info(\"\\n2.2: Generate Additional Cohorts\")\n",
    "def and_log(s):\n",
    "    logging.info(s)\n",
    "    return s\n",
    "\n",
    "# if the analysis failed, create (if necessary) the flag file and\n",
    "# add to it the reason it failed; increase max fail level if necessary\n",
    "def mark_analysis_failed(text, level):\n",
    "    try:\n",
    "        with open(c[\"file\"][\"flag_analysis_failed\"], \"r\") as jf:\n",
    "            failed_json = json.load(jf)\n",
    "    except IOError, e:\n",
    "        if e.errno == errno.ENOENT:\n",
    "            failed_json = {\"reason\": {}, \"maxlevel\": str(level)}\n",
    "        else:\n",
    "            raise\n",
    "    if int(failed_json[\"maxlevel\"]) < level:\n",
    "        failed_json[\"maxlevel\"] = str(level)\n",
    "    if \"2.2\" in failed_json[\"reason\"].keys():\n",
    "        failed_json[\"reason\"][\"2.2\"] = failed_json[\"reason\"][\"2.2\"] + text\n",
    "    else:\n",
    "        failed_json[\"reason\"][\"2.2\"] = text\n",
    "    with open(c[\"file\"][\"flag_analysis_failed\"], \"w\") as jf:\n",
    "        json.dump(failed_json, jf, indent=2)\n",
    "\n",
    "\n",
    "j = {}\n",
    "\n",
    "# Input requires steps: 2.0\n",
    "with open(c[\"json\"][\"2.0\"],\"r\") as jf:\n",
    "        json_2pt0 = json.load(jf)\n",
    "        \n",
    "compendium_sample_alias=c[\"info\"][\"id_for_tumormap\"]\n",
    "\n",
    "if(compendium_sample_alias != sample_id):\n",
    "    print \"Using the alias '{}' when searching for this sample on the compendium.\".format(compendium_sample_alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First and second degree most correlated sample cohort.\n",
    "\n",
    "Use the all-by-all matrix to get the second degree most correlated samples.\n",
    "For each first degree sample S, get all samples which have a correlation to S of the similarity threshold or greater. \n",
    "\n",
    "Note that this includes S itself (with a correlation of 1), so we get all the first-degree samples as we aggregate the second-degree ones.\n",
    "\n",
    "We use the cohort correlation threshold rather than tumormap as we are only working with cohort samples for these second degree samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_by_all=pd.read_csv(c[\"cohort\"][\"all_by_all_tsv\"],\n",
    "                       delimiter=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "threshold = float(c[\"cohort\"][\"info\"][\"mcs_similarity_threshold\"])\n",
    "first_and_second_degree_samples = set()\n",
    "\n",
    "# TODO: The first-degree samples are retrieved from the tumormap cohort.\n",
    "# If any of them are not present in the outlier pancancer cohort, this section will crash with a KeyError.\n",
    "\n",
    "for first in json_2pt0['first_degree_mcs_cohort']:\n",
    "    firsts_mcs = all_by_all[all_by_all[first] >= threshold].index\n",
    "    first_and_second_degree_samples = first_and_second_degree_samples.union(firsts_mcs)\n",
    "    print \"First-degree sample {} has {} second-degree MCS above threshold\".format(first, len(firsts_mcs) - 1)\n",
    "    \n",
    "j[\"first_and_second_degree_mcs_cohort\"] = sorted(first_and_second_degree_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cohort: diagnosed disease.\n",
    "\n",
    "Retrieve the N-of-1 sample's disease (when provided) and get all background samples that match it.\n",
    "\n",
    "Then, when a roll-up cohort is provided, use that in preference to any diagnosis cohort; otherwise, use the collected samples as the n-of-1 disease cohort. (Dropping the N-of-1 sample subsequently when present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cohort_diseases = pd.read_csv(\n",
    "    c[\"cohort\"][\"essential_clinical\"], \n",
    "    sep=\"\\t\", keep_default_na=False, na_values=['_'])\n",
    "\n",
    "# If there's a disease, get the samples with that disease\n",
    "if(c[\"info\"][\"disease\"]):\n",
    "    same_disease = cohort_diseases[\n",
    "        cohort_diseases[\"disease\"] == c[\"info\"][\"disease\"]]\n",
    "    samples_same_disease = sorted(list(same_disease[\"th_sampleid\"]))\n",
    "else:\n",
    "    print \"No disease was found for {}.\".format(sample_id)\n",
    "    samples_same_disease = []\n",
    "\n",
    "# Get count of same-disease samples for future work (INCLUDING n-of-1 sample if present)\n",
    "# this may be different from len of diagnosed disease cohort if a roll-up cohort is being used.\n",
    "j[\"count_of_samples_same_disease\"] = str(len(samples_same_disease)     )\n",
    "\n",
    "# Then, figure out what the diagnosed disease cohort is.\n",
    "# It's either the roll-up cohort, the samples w same disease, or empty.\n",
    "if c[\"info\"][\"rollup\"]:\n",
    "    j[\"diagnosed_disease_cohort\"] = sorted(list(c[\"info\"][\"rollup\"]))\n",
    "    print \"Using provided roll-up cohort with {} samples.\".format(len(j[\"diagnosed_disease_cohort\"]))\n",
    "    \n",
    "elif samples_same_disease:\n",
    "    j[\"diagnosed_disease_cohort\"] = samples_same_disease\n",
    "    print \"Found disease '{}'. Generating cohort with {} samples.\".format(\n",
    "        c[\"info\"][\"disease\"], len(j[\"diagnosed_disease_cohort\"]))\n",
    "    \n",
    "else:\n",
    "    print (\"No samples were found matching the focus sample's disease, and a roll-up cohort was not provided.\"\n",
    "           \"The diagnosed-disease cohort will be empty.\")\n",
    "    j[\"diagnosed_disease_cohort\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, drop the N-of-1 self-sample, as identified by id (or alias on the compendium if present), from these cohorts.\n",
    "\n",
    "If the diagnosed disease cohort now has fewer than 20 samples, we'll omit it as being insufficient. (Roll-up cohorts with fewer than 20 samples are NOT omitted, but the analyst is alerted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MINIMUM_DIAGNOSIS_COHORT_THRESHOLD = 20\n",
    "\n",
    "try:\n",
    "    j[\"diagnosed_disease_cohort\"].remove(compendium_sample_alias)\n",
    "    ddc_str = \"removed from\"\n",
    "except ValueError:\n",
    "    ddc_str = \"not found in\"\n",
    "\n",
    "try:\n",
    "    j[\"first_and_second_degree_mcs_cohort\"].remove(compendium_sample_alias)\n",
    "    fsd_str = \"removed from\"\n",
    "except ValueError:\n",
    "    fsd_str = \"not found in\"\n",
    "\n",
    "print \"N-of-1 sample {} was {} the diagnosed-disease cohort [Alias: {}]\".format(sample_id,\n",
    "                                                                                ddc_str,\n",
    "                                                                                compendium_sample_alias)\n",
    "\n",
    "print \"N-of-1 sample {} was {} the first-and-second-degree-MCS cohort [Alias {}]\".format(sample_id,\n",
    "                                                                                        fsd_str,\n",
    "                                                                                        compendium_sample_alias)\n",
    "\n",
    "if (len(j[\"diagnosed_disease_cohort\"]) < MINIMUM_DIAGNOSIS_COHORT_THRESHOLD):\n",
    "    if c[\"info\"][\"rollup\"]:\n",
    "        rollup_cohort_too_small_message = (\n",
    "            \"The provided roll-up cohort has {} samples; this is below the minimum size of {}. \".format(\n",
    "            len(j[\"diagnosed_disease_cohort\"]), MINIMUM_DIAGNOSIS_COHORT_THRESHOLD) +\n",
    "            \"This cohort has been used in analysis but its size may be too small to provide meaningful results.\"\n",
    "        )\n",
    "        mark_analysis_failed(rollup_cohort_too_small_message, 1)\n",
    "        print rollup_cohort_too_small_message\n",
    "    else:\n",
    "        diagnosis_cohort_too_small_message = (\n",
    "            \"The N-of-1 disease cohort has {} samples; this is below the minimum size of {}. \".format(\n",
    "            len(j[\"diagnosed_disease_cohort\"]), MINIMUM_DIAGNOSIS_COHORT_THRESHOLD) +\n",
    "            \"This cohort was omitted. Provide a roll-up cohort in the manifest.tsv for this sample and rerun.\"\n",
    "        )\n",
    "        mark_analysis_failed(diagnosis_cohort_too_small_message, 4)\n",
    "        print diagnosis_cohort_too_small_message\n",
    "        j[\"diagnosed_disease_cohort\"] = []\n",
    "else:\n",
    "    print \"\\nBuilt a diagnosed-disease cohort with {} sample IDs.\".format(len(j[\"diagnosed_disease_cohort\"]))\n",
    "    \n",
    "print \"Built a first-and-second-degree-MCS cohort with {} sample IDs.\".format(\n",
    "    len(j[\"first_and_second_degree_mcs_cohort\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the tumormap legend for step 9: Take the union of all samples in all personalized cohorts, get all diseases, and add those to the legend.\n",
    "\n",
    "Also store the full legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Personalized cohorts:\n",
    "all_cohorts = [\n",
    "    #json_2pt0['first_degree_mcs_cohort'], # this is redundant with the 1st-and-2nd cohort in this context\n",
    "    json_2pt0[\"pandisease_samples\"],\n",
    "    j[\"first_and_second_degree_mcs_cohort\"],\n",
    "    j[\"diagnosed_disease_cohort\"]\n",
    "]\n",
    "all_neighbors = set()\n",
    "for cohort in all_cohorts:\n",
    "    all_neighbors = all_neighbors.union(cohort)\n",
    "\n",
    "diseases_df = cohort_diseases[\n",
    "    cohort_diseases[\"th_sampleid\"].isin(all_neighbors)][[\"th_sampleid\", \"disease\"]]\n",
    "all_found_diseases =  filter(lambda x: x != \"\", diseases_df[\"disease\"].unique().tolist())\n",
    "\n",
    "\n",
    "j[\"mss_disease_colors\"] = {}\n",
    "\n",
    "try:\n",
    "    with open(c[\"tumormap\"][\"disease_color_map\"], \"r\") as f:\n",
    "        disease_colors = json.load(f)\n",
    "except IOError:\n",
    "    j[\"tumormap_legend\"] = {}\n",
    "    j[\"mss_disease_colors\"] = {}\n",
    "else:\n",
    "    j[\"tumormap_legend\"] = disease_colors\n",
    "    for disease_name in all_found_diseases:\n",
    "        try:\n",
    "            j[\"mss_disease_colors\"][disease_name] = disease_colors[disease_name]\n",
    "        except KeyError:\n",
    "            print(\"Didn't find {} in map {} - skipping!\".format(disease_name, c[\"tumormap\"][\"disease_color_map\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get disease counts for each of the personalized cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_cohorts = {\n",
    "    \"first_degree_mcs_cohort\":json_2pt0['first_degree_mcs_cohort'],\n",
    "    \"pandisease_samples\":json_2pt0[\"pandisease_samples\"],\n",
    "    \"first_and_second_degree_mcs_cohort\": j[\"first_and_second_degree_mcs_cohort\"],\n",
    "    \"diagnosed_disease_cohort\": j[\"diagnosed_disease_cohort\"]\n",
    "}\n",
    "j[\"personalized_cohort_counts\"] = {}\n",
    "\n",
    "for (cohort, samples) in all_cohorts.iteritems():\n",
    "    disease_items = cohort_diseases[cohort_diseases[\"th_sampleid\"].isin(samples)][\"disease\"]\n",
    "    j[\"personalized_cohort_counts\"][cohort] = {\"total\" : len(disease_items)}\n",
    "    j[\"personalized_cohort_counts\"][cohort][\"diseases\"] = dict(zip(*np.unique(disease_items, return_counts=True)))\n",
    "\n",
    "j[\"personalized_cohort_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# json output.\n",
    "# Keys are:\n",
    "# diagnosed_disease_cohort: array of sample IDs\n",
    "# first_and_second_degree_mcs_cohort: array of sample IDs\n",
    "\n",
    "with open(c[\"json\"][\"2.2\"], \"w\") as jsonfile:\n",
    "    json.dump(j, jsonfile, indent=2)\n",
    "    \n",
    "print \"Done!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
