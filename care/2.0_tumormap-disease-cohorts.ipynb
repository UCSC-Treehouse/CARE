{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Use the tumormap algorithm to calculate the most similar samples\n",
    "- calculate disease from those samples\n",
    "- write the pancancer and pandisease sample lists for this sample\n",
    "\n",
    "### Depends On Steps:\n",
    "    0 (conf)\n",
    "    1 (sample expression file created)\n",
    "\n",
    "### Input files:\n",
    "\n",
    "Tumormap placement is run using the `tumormap_expression` file within the `tumormap/` subdir of the compendium.\n",
    "This file can be either .tsv or .hd5 format. If both are present, the .tsv will be used.\n",
    "\n",
    "The standard setup is to have only a `tumormap_expression.hd5` file which is a hardlink to the outlier cohort hd5 file, and a `filtered_genes_to_keep.tsv` file listing the genes in that file to retain.  \n",
    "\n",
    "The alternate setup is to have only a `tumormap_expression.tsv` file, in which case the `filtered_genes_to_keep.tsv` is not used.\n",
    "\n",
    "These input files that are always used:\n",
    "- c[\"cohort\"][\"essential_clinical\"] - the clinical data for the outlier analysis cohort; only the disease column is used.\n",
    "- c[\"tumormap\"][\"essential_clinical\"] - the clinical data for the samples in the Tumormap background cohort. In the standard setup, these are the same as the outlier cohort samples. In the alternate setup, they might be a different set of samples. \n",
    "\n",
    "These input files are sometimes used:\n",
    "- c[\"tumormap\"][\"filtered_genes_to_keep\"] - Used in the standard setup. List of genes to retain in the hd5 file based on the results of the expression & variance filters. \n",
    "\n",
    "- c[\"tumormap\"][\"background_hdf\"] - Used in the standard setup. Path to the hd5 file of background expression to place the n-of-1 sample on.\n",
    "- c[\"tumormap\"][\"background_tsv\"] - Used in the alternate setup. Path to the tsv file of background expression. filtered_genes_to_keep are not checked for this file; it's expected to have expression & variance filters pre-applied.\n",
    "- c[\"file\"][\"alternate_tumormap_expression\"] - Path to an alternate N-of-1 expression file that should be used for tumormap placement instead of the calculated N-of-1 expression file. Will be used instead of the calculated n-of-1 expression if a file exists at this path.\n",
    "\n",
    "### Output files:\n",
    "\n",
    "j[\"tumormap_results\"] : a { neighbor ID -> similarity } dict\n",
    "    - used by 2.5, 2.7\n",
    "j[\"neighbor_diseases\"] : array starting with sample ID\n",
    "\n",
    "### Details\n",
    "\n",
    "#### Tumormap Algorithm\n",
    "The nearest_samples() function computes the most similar samples to our N-of-1 sample. The code \n",
    "of this function was extracted from Yulia Newton's compute_sparse_matrix.py script.\n",
    "\n",
    "The following version of compute_sparse_matrix.py was used as the source:\n",
    "\n",
    "https://github.com/ucscHexmap/compute/blob/254838415da027039d5b2102579d18ea0e115438/calc/compute_sparse_matrix.py\n",
    "\n",
    "#### Fully qualified sample ID\n",
    "The \"fully qualified sample ID\" is relevant when our N-of-1 sample is already present in the background cohort.\n",
    "It is not relevant for placing on the tumormap, in which the N-of-1 sample ID is not important; but it is relevant for pre- and post-processing.\n",
    "\n",
    "If our sample is present in the cohort, it's expected to appear in the MCS with a 1.0 correlation. Thus, we add an additional MCS slot beforehand to account for this, and check that it appeared as expected afterward.\n",
    "\n",
    "The tumormap results will appear using the actual sample ID, and not the FQsample ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics.pairwise as sklp\n",
    "import operator\n",
    "\n",
    "# Setup: load conf, retrieve sample ID, logging\n",
    "with open(\"conf.json\",\"r\") as conf:\n",
    "    c=json.load(conf)\n",
    "sample_id = c[\"sample_id\"]    \n",
    "print(\"Running on sample: {}\".format(sample_id))\n",
    "\n",
    "logging.basicConfig(**c[\"info\"][\"logging_config\"])\n",
    "logging.info(\"\\n2.0: Get Most Similar Samples\")\n",
    "def and_log(s):\n",
    "    logging.info(s)\n",
    "    return s\n",
    "    \n",
    "# if the analysis failed, create (if necessary) the flag file and\n",
    "# add to it the reason it failed; increase max fail level if necessary\n",
    "def mark_analysis_failed(text, level):\n",
    "    try:\n",
    "        with open(c[\"file\"][\"flag_analysis_failed\"], \"r\") as jf:\n",
    "            failed_json = json.load(jf)\n",
    "    except IOError, e:\n",
    "        if e.errno == errno.ENOENT:\n",
    "            failed_json = {\"reason\": {}, \"maxlevel\": str(level)}\n",
    "        else:\n",
    "            raise\n",
    "    if int(failed_json[\"maxlevel\"]) < level:\n",
    "        failed_json[\"maxlevel\"] = str(level)\n",
    "    if \"2.0\" in failed_json[\"reason\"].keys():\n",
    "        failed_json[\"reason\"][\"2.0\"] = failed_json[\"reason\"][\"2.0\"] + text\n",
    "    else:\n",
    "        failed_json[\"reason\"][\"2.0\"] = text\n",
    "    with open(c[\"file\"][\"flag_analysis_failed\"], \"w\") as jf:\n",
    "        json.dump(failed_json, jf, indent=2)\n",
    "\n",
    "j = {}\n",
    "\n",
    "\n",
    "fqsample_id=c[\"info\"][\"id_for_tumormap\"]\n",
    "if(fqsample_id != sample_id):\n",
    "    print \"Using the alias '{}' when searching for this sample on the compendium.\".format(fqsample_id)\n",
    "    \n",
    "### Configuration ###\n",
    "neighbor_count=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the minimum threshold of correlation for which a sample is sufficiently similar to our N-of-1 sample. Use the tumormap threshold (vs cohort) as we're pulling these MCS from the tumormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SIMILARITY_THRESHOLD = float(c[\"tumormap\"][\"info\"][\"mcs_similarity_threshold\"])\n",
    "print SIMILARITY_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Figure out which cohort we are using -- either the standard hd5 or the alternate tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# if expression TSV exists :\n",
    "# use it, don't apply filter\n",
    "# otherwise, use expression hd5, apply filter.\n",
    "\n",
    "if(os.path.isfile(c[\"tumormap\"][\"background_tsv\"])):\n",
    "    print (\"Loading tumormap background cohort as TSV.\")\n",
    "    print (\"Expecting that expression & variance filters have been pre-applied.\")\n",
    "    filtered_cohort = pd.read_csv(c[\"tumormap\"][\"background_tsv\"],\n",
    "                         sep=\"\\t\",\n",
    "                         index_col=0)\n",
    "else:\n",
    "    print (\"Loading tumormap background cohort as hdf.\")\n",
    "    cohort = pd.read_hdf(c[\"tumormap\"][\"background_hdf\"])\n",
    "    print (\"Filtering genes based on expression and variance filters.\")\n",
    "    \n",
    "    keep_these_genes=pd.read_csv(c[\"tumormap\"][\"filtered_genes_to_keep\"],\n",
    "                             sep=\"\\t\", dtype=\"str\", index_col=\"Gene\").index\n",
    "    filtered_cohort = cohort.loc[keep_these_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the Spearman distance between sample and every member of the\n",
    "cohort and return the N nearest samples from the cohort.\n",
    "\n",
    "cohort, sample: Pandas data frames with rows=genes\n",
    "N: the number of most similar samples to return\n",
    "\n",
    "\"\"\"\n",
    "def nearest_samples(cohort, sample, N=-1):\n",
    "    # Reduce to only common features\n",
    "    print(\"Computing intersection\")\n",
    "    intersection = cohort.index.intersection(sample.index)\n",
    "    if len(intersection) == 0:\n",
    "        print(\"Error: sample and cohort have no features in common.\\n\"\n",
    "              \"Are you using an alternate cohort and need to use the alternate n-of-1 sample?\")\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "    cohort_incommon = cohort[cohort.index.isin(intersection)].sort_index(axis=0)\n",
    "    sample_incommon = sample[sample.index.isin(intersection)].sort_index(axis=0)\n",
    "    \n",
    "    # Column wise rank transform to turn correlation into spearman\n",
    "    print(\"Transforming Rank\")\n",
    "    cohort_transformed = np.apply_along_axis(scipy.stats.rankdata, 1, cohort_incommon.values.T)\n",
    "    sample_transformed = np.apply_along_axis(scipy.stats.rankdata, 1, sample_incommon.values.T)\n",
    "\n",
    "    # Compute spearman distances\n",
    "    print(\"Computing distances\")\n",
    "    distances = sklp.pairwise_distances(X=cohort_transformed, Y=sample_transformed, metric=\"correlation\", n_jobs=1)\n",
    "            \n",
    "    # Rank and return top N\n",
    "    rank = 1 - pd.DataFrame(distances, cohort.columns.values)\n",
    "    return rank.sort_values(by=0, ascending=False)[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Should we use a pre-existing alternate expression file, or our calculated one?\n",
    "if(os.path.exists(c[\"file\"][\"alternate_tumormap_expression\"])):\n",
    "    print(\"Loading alternate tumormap expression from {}\".format(c[\"file\"][\"alternate_tumormap_expression\"]))\n",
    "    n_of_1_expression=pd.read_csv(c[\"file\"][\"alternate_tumormap_expression\"], delimiter=\"\\t\", index_col=0)\n",
    "else:\n",
    "    print(\"Loading sample expression from Step 1 JSON file\")\n",
    "    with open(c[\"json\"][\"1\"],\"r\") as step1_json:\n",
    "        n_of_1_expression=pd.DataFrame.from_dict(json.load(step1_json)[\"tpm_hugo_norm_uniq\"],\n",
    "                                                 dtype=\"float64\", orient=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, get the correlations from our N-of-1 sample to every other sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cohort_correlations = nearest_samples(filtered_cohort, n_of_1_expression, N=len(filtered_cohort.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the traditional tumormap results. This is the top 6 samples, or, if the N-of-1 sample is in the tumormap background cohort, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the tumormap metadata & see whether our N-of-1 sample is in it\n",
    "tumormap_diseases = pd.read_csv(\n",
    "                    c[\"tumormap\"][\"essential_clinical\"], \n",
    "                    sep=\"\\t\", keep_default_na=False, na_values=['_']) # no default NA so we get \"\" instead of np.nan\n",
    "self_neighbor_expected = int(fqsample_id in tumormap_diseases[\"th_sampleid\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Traditional tumormap results, retaining the self-sample\n",
    "neighbor_ids_and_values = cohort_correlations[\n",
    "    0:(neighbor_count + self_neighbor_expected)].to_dict()[0]\n",
    "j[\"tumormap_results\"] = neighbor_ids_and_values.copy()\n",
    "neighbor_ids_and_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Then, handle potential error cases before we continue on with the analysis, checking the entire correlation results. This will drop an existing self-sample from correlations_dict.\n",
    "\n",
    "OK cases:\n",
    "  - We didn't expect the self-sample in the MSS, and it is not\n",
    "  - We expected the self-sample in the MSS, and it is, with good correlation ( > .999)\n",
    "  \n",
    "Failure cases:\n",
    "  - There is a sample in the MSS with good correlation ( > .999) that we did not expect\n",
    "\n",
    "  - We expected the self-sample in the MSS, and it is there, but with bad correlation ( < .999)\n",
    "  - We expected the self-sample in the MSS, and it is not there at all\n",
    "  \n",
    "  Also, make a failure if no MSS (other than self) has a correlation higher than 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correlations_dict = cohort_correlations.to_dict()[0]\n",
    "\n",
    "# If we added a neighbor for itself, confirm that the self neighbor was found.\n",
    "# Then drop it from neighbor_ids_and_values so that its disease will not be included.\n",
    "if(self_neighbor_expected == 1):\n",
    "    if(fqsample_id in correlations_dict):\n",
    "        self_corr = correlations_dict.pop(fqsample_id)\n",
    "        if(self_corr < 0.999): # Correlation arbitrarily chosen\n",
    "            lowself_message = (\"This sample was indicated to be present on the Tumor Map background cohort\"\n",
    "                             \" with ID {}. However, the sample with that ID has only {} correlation with\"\n",
    "                             \" the focus sample. Confirm that the sample is labeled correctly.<br/><br/>\").format(\n",
    "                             fqsample_id, self_corr)\n",
    "            print lowself_message\n",
    "            mark_analysis_failed(lowself_message, 4)\n",
    "    else:\n",
    "        notfound_message = (\"This sample was indicated to be present on the Tumor Map background cohort\"\n",
    "                            \" with ID {}. However, none of the sample's Most Correlated Samples have that ID.\"\n",
    "                            \" You might have the wrong focus sample expression file!\"\n",
    "                            \" Confirm that the sample is labeled correctly.<br/><br/>\").format(fqsample_id)\n",
    "        print notfound_message\n",
    "        mark_analysis_failed(notfound_message, 4)\n",
    "        \n",
    "# On the other hand, make sure that we didn't get a self neighbor we weren't expecting\n",
    "else:\n",
    "    for neighbor, value in correlations_dict.iteritems():\n",
    "        if value > 0.999:\n",
    "            tooclose_message = (\"Most Correlated Sample correlation too high! The background sample {} \"\n",
    "                               \"has a {} correlation with the focus sample; this might be the same expression\"\n",
    "                               \" file. If this is intentional, use the alias column in the manifest.tsv to indicate \"\n",
    "                               \"that the focus sample is present in the cohort under this name.<br/><br/>\").format(\n",
    "                neighbor, value)\n",
    "            print(tooclose_message)\n",
    "            mark_analysis_failed(tooclose_message, 4)\n",
    "            \n",
    "if max(correlations_dict.values()) <= 0.8:\n",
    "    lowcorr_message = (\"Most Correlated Sample correlation too low! The highest correlation for this sample is \"\n",
    "                      \"{}. A correlation below 0.80 can indicate extremely low read depth, incorrect normalization, \"\n",
    "                      \" or other problems.<br/><br/>\").format(max(correlations_dict.values()))\n",
    "    print(lowcorr_message)\n",
    "    mark_analysis_failed(lowcorr_message, 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Correlations dict (now sans focus sample) added to output json.\n",
    "j[\"correlations_vs_focus_sample\"] = correlations_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's make some personalized cohorts based on the tumormap results.\n",
    "First, get all the first-degree Most Correlated Samples: all samples which are more similar to the N-of-1 sample than the similarity threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_degree_mcs = {k:v for (k,v) in correlations_dict.items() if v > SIMILARITY_THRESHOLD}\n",
    "j[\"first_degree_mcs_cohort\"] = sorted(first_degree_mcs.keys())\n",
    "# TODO: If any of these samples are not in the outlier cohort, a later step will fail.\n",
    "first_degree_mcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, we'll make our new variant on traditional pan-disease cohort. This are the diseases of the top 6 first-degree MCS; that is, the typical top 6 neighbors, but only counting those which are more similar than the threshold.\n",
    "If we have fewer than 6, use only those.\n",
    "\n",
    "The self-sample has already been dropped from correlations_dict, so we simply use neighbor_count without self_neighbor_expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top_six_ids = sorted(\n",
    "    first_degree_mcs, key=first_degree_mcs.get, reverse=True)[0:neighbor_count]\n",
    "top_six_dict = { k: first_degree_mcs[k] for k in top_six_ids }\n",
    "j[\"tumormap_results_above_threshold\"] = top_six_dict.copy()\n",
    "\n",
    "# Find the diseases associated with the neighbors\n",
    "dis_samps_df = tumormap_diseases[tumormap_diseases[\"th_sampleid\"].isin(top_six_dict)]\n",
    "print \"Neighbors and Diseases:\"\n",
    "print dis_samps_df[[\"th_sampleid\", \"disease\"]]\n",
    "\n",
    "# Get diseases and drop empty\n",
    "cleaned_found_diseases =  filter(lambda x: x != \"\", dis_samps_df[\"disease\"].unique().tolist())\n",
    "if len(cleaned_found_diseases) == 0:\n",
    "    print \"Warning! No diseases found for any nearest neighbor! Pandisease cohort is empty.\"\n",
    "    # The pandisease cohort will be empty, but we may be able to survive with other cohorts\n",
    "else:\n",
    "    print \"\\nUsing the following diseases:\\n{}\".format(\"\\n\".join(cleaned_found_diseases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the sample lists for the pancancer and disease-specific cohorts\n",
    "# Load up the samples vs disease matrix for the Compendium (not tumormap)\n",
    "cohort_diseases = pd.read_csv(\n",
    "                    c[\"cohort\"][\"essential_clinical\"], \n",
    "                    sep=\"\\t\", keep_default_na=False, na_values=['_'])\n",
    "\n",
    "# Exclude the sample in question (including medbook name if present) from the cohort\n",
    "samples_excluding_self = cohort_diseases[~cohort_diseases[\"th_sampleid\"].isin([fqsample_id])]\n",
    "\n",
    "# Then, select the samples for the diseases that were found among the neighbors\n",
    "# This selects from the pool of samples available to the cohort, as opposed to those available from the tumormap.\n",
    "\n",
    "disease_specific_df = samples_excluding_self[samples_excluding_self [\"disease\"].isin(cleaned_found_diseases)]\n",
    "\n",
    "j[\"pancan_samples\"] = sorted(samples_excluding_self[\"th_sampleid\"].tolist())\n",
    "j[\"pandisease_samples\"] = sorted(disease_specific_df[\"th_sampleid\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print some useful info here\n",
    "\n",
    "if(fqsample_id in cohort_diseases[\"th_sampleid\"].values):\n",
    "    isin = \"\"\n",
    "    hasbeenremoved = \" and has been removed\"\n",
    "else:\n",
    "    isin = \" not\"\n",
    "    hasbeenremoved = \"\"\n",
    "\n",
    "print(\"Background cohorts generated.\")\n",
    "print(\"Original cohort: {} samples\".format(len(cohort_diseases)))\n",
    "print(\"N-of-1 sample {} was{} in the original cohort{}.\".format(fqsample_id, isin, hasbeenremoved))\n",
    "print(\"{} total pancancer samples.\".format(len(j[\"pancan_samples\"])))\n",
    "print(\"Diseases in pandisease cohort: {}\".format(\", \".join(cleaned_found_diseases)))\n",
    "print(\"{} total pandisease samples.\".format(len(j[\"pandisease_samples\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For each disease, count how many samples in the pandisease cohort are listed with this disease\n",
    "# Used by the report in step 2.5\n",
    "j[\"sample_disease_counts\"]=(\"Pan-disease cohort breakdown:\\nDisease\\tCohort_sample_count\\n\" +\n",
    "                           \"\\n\".join(\n",
    "                                map(\n",
    "                                    lambda disease: \"{}\\t{}\".format(\n",
    "                                        disease,\n",
    "                                        len(\n",
    "                                            disease_specific_df[\n",
    "                                                disease_specific_df[\"disease\"].isin([disease])])),\n",
    "                                    cleaned_found_diseases\n",
    "                            )))\n",
    "\n",
    "print j[\"sample_disease_counts\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a TSV of compendium correlations vs focus sample, suitable to load into Tumormap as an attribute. Sorted by correlation from high to low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(c[\"file\"][\"sample_vs_compendium_tumormap_attribute\"], \"w\") as f:\n",
    "    f.write(\"sample ID\\tCorrelationVs{}\\n\".format(c[\"sample_id\"]))\n",
    "    for k, v in sorted(j[\"correlations_vs_focus_sample\"].iteritems(),\n",
    "                       key=operator.itemgetter(1),\n",
    "                       reverse=True):\n",
    "        f.write(\"{}\\t{}\\n\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the final JSON\n",
    "\n",
    "with open(c[\"json\"][\"2.0\"], \"w\") as jsonfile:\n",
    "    json.dump(j, jsonfile, indent=2)\n",
    "    \n",
    "print \"Done!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
